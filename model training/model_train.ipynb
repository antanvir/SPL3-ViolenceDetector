{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_save.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpUImZuKMkJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a7677e-d058-40aa-fb2c-0fe79a807da2"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k754qfuOlTa",
        "outputId": "b8720cbf-f1a1-4d84-d0bb-7ab61f54ed1f"
      },
      "source": [
        "import pickle\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "x_validation_data_path = [\r\n",
        "                       \"/content/drive/My Drive/_SPL3/model/x_hockey_validation.pickle\",\r\n",
        "                       \"/content/drive/My Drive/_SPL3/model/x_movie_validation.pickle\",\r\n",
        "                       \"/content/drive/My Drive/_SPL3/model/x_crowd_validation.pickle\"\r\n",
        "                    ]\r\n",
        "y_validation_data_path = [\r\n",
        "                       \"/content/drive/My Drive/_SPL3/model/y_hockey_validation.pickle\",\r\n",
        "                       \"/content/drive/My Drive/_SPL3/model/y_movie_validation.pickle\",\r\n",
        "                       \"/content/drive/My Drive/_SPL3/model/y_crowd_validation.pickle\"\r\n",
        "                    ]\r\n",
        "x_data_path = [\r\n",
        "                       \"/content/drive/My Drive/_SPL3/model/x_hockey.pickle\",\r\n",
        "                       \"/content/drive/My Drive/_SPL3/model/x_movie.pickle\",\r\n",
        "                       \"/content/drive/My Drive/_SPL3/model/x_crowd.pickle\"\r\n",
        "                    ]\r\n",
        "y_data_path = [\r\n",
        "                       \"/content/drive/My Drive/_SPL3/model/y_hockey.pickle\",\r\n",
        "                       \"/content/drive/My Drive/_SPL3/model/y_movie.pickle\",\r\n",
        "                       \"/content/drive/My Drive/_SPL3/model/y_crowd.pickle\"\r\n",
        "                    ]\r\n",
        "\r\n",
        "# loading saved dataset with pickle\r\n",
        "x_validation, y_validation, x, y = None, None, None, None\r\n",
        "for i in range(0, len(x_data_path)):\r\n",
        "    pickle_input_x_validation = open(x_validation_data_path[i], \"rb\")\r\n",
        "    # x_validation = pickle.load(pickle_input_x_validation)\r\n",
        "    new_x_valid = pickle.load(pickle_input_x_validation)\r\n",
        "    pickle_input_x_validation.close()\r\n",
        "\r\n",
        "    pickle_input_y_validation = open(y_validation_data_path[i], \"rb\")\r\n",
        "    new_y_valid = pickle.load(pickle_input_y_validation)\r\n",
        "    pickle_input_y_validation.close()\r\n",
        "\r\n",
        "    pickle_input_x = open(x_data_path[i], \"rb\")\r\n",
        "    new_x = pickle.load(pickle_input_x)\r\n",
        "    pickle_input_x.close()\r\n",
        "\r\n",
        "    pickle_input_y = open(y_data_path[i], \"rb\")\r\n",
        "    new_y = pickle.load(pickle_input_y)\r\n",
        "    pickle_input_y.close()\r\n",
        "\r\n",
        "    if i == 0:\r\n",
        "        x_validation = new_x_valid\r\n",
        "        y_validation = new_y_valid\r\n",
        "        x = new_x\r\n",
        "        y = new_y\r\n",
        "    else:\r\n",
        "        x_validation = np.concatenate( (x_validation, new_x_valid) )\r\n",
        "        y_validation = np.concatenate( (y_validation, new_y_valid) )\r\n",
        "        x = np.concatenate( (x, new_x) )\r\n",
        "        y = np.concatenate( (y, new_y) )\r\n",
        "    print(\"Training Set: New=\", new_x.shape, \"\\tMerged=\", x.shape)\r\n",
        "    print(\"Validation Set: New=\", new_x_valid.shape, \"\\tMerged=\", x_validation.shape, \"\\n\")\r\n",
        "\r\n",
        "\r\n",
        "x_validation = x_validation.astype(dtype=np.float32)/255\r\n",
        "y_validation = np.array(y_validation)\r\n",
        "x = x.astype(dtype=np.float32)/255\r\n",
        "y = np.array(y)\r\n",
        "\r\n",
        "# Taking Dump of The Complete Merged Dataset\r\n",
        "pickle_write_x = open(\"/content/drive/My Drive/_SPL3/model/x_validation_all.pickle\", \"wb\")\r\n",
        "pickle.dump(x_validation, pickle_write_x)\r\n",
        "pickle_write_x.close()\r\n",
        "\r\n",
        "pickle_write_y = open(\"/content/drive/My Drive/_SPL3/model/y_validation_all.pickle\", \"wb\")\r\n",
        "pickle.dump(y_validation, pickle_write_y)\r\n",
        "pickle_write_y.close()\r\n",
        "\r\n",
        "pickle_write_x = open(\"/content/drive/My Drive/_SPL3/model/x_all.pickle\", \"wb\")\r\n",
        "pickle.dump(x, pickle_write_x)\r\n",
        "pickle_write_x.close()\r\n",
        "\r\n",
        "pickle_write_y = open(\"/content/drive/My Drive/_SPL3/model/y_all.pickle\", \"wb\")\r\n",
        "pickle.dump(y, pickle_write_y)\r\n",
        "pickle_write_y.close()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set: New= (1810, 10, 100, 100, 3) \tMerged= (1810, 10, 100, 100, 3)\n",
            "Validation Set: New= (204, 10, 100, 100, 3) \tMerged= (204, 10, 100, 100, 3) \n",
            "\n",
            "Training Set: New= (435, 10, 100, 100, 3) \tMerged= (2245, 10, 100, 100, 3)\n",
            "Validation Set: New= (48, 10, 100, 100, 3) \tMerged= (252, 10, 100, 100, 3) \n",
            "\n",
            "Training Set: New= (983, 10, 100, 100, 3) \tMerged= (3228, 10, 100, 100, 3)\n",
            "Validation Set: New= (112, 10, 100, 100, 3) \tMerged= (364, 10, 100, 100, 3) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFsmMHBAck8Q"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from keras import backend as K\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, TimeDistributed,Dropout, Activation, Flatten,Conv2D, MaxPooling2D,LSTM,Bidirectional\r\n",
        "from tensorflow.keras.optimizers import Adam, SGD\r\n",
        "from tensorflow.keras.callbacks import TensorBoard\r\n",
        "\r\n",
        "K.set_image_data_format('channels_last')\r\n",
        "NAME = \"VIOLENCE-DETECTOR\"\r\n",
        "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\r\n",
        "\r\n",
        "## training the CNN\r\n",
        "cnn = Sequential()\r\n",
        "#input\r\n",
        "cnn.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\r\n",
        "#1st layer\r\n",
        "cnn.add(Conv2D(64, (3, 3), activation='relu'))\r\n",
        "cnn.add(MaxPooling2D((2, 2)))\r\n",
        "cnn.add(Conv2D(64, (3, 3), activation='relu'))\r\n",
        "cnn.add(MaxPooling2D((2, 2)))\r\n",
        "cnn.add(Conv2D(64, (3, 3), activation='relu'))\r\n",
        "cnn.add(MaxPooling2D((2, 2)))\r\n",
        "#converting to 1-d tensor\r\n",
        "cnn.add(Flatten())\r\n",
        "\r\n",
        "model=Sequential()\r\n",
        "model.add(TimeDistributed(cnn,input_shape=x.shape[1:]))\r\n",
        "model.add(Bidirectional(LSTM(32)))\r\n",
        "#model.add(LSTM(32))\r\n",
        "model.add(Dense(64,activation='relu'))\r\n",
        "model.add(Dense(32,activation='relu'))\r\n",
        "#model.add(Flatten())\r\n",
        "model.add(Dense(2,activation='sigmoid'))\r\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\r\n",
        "print(model.summary())\r\n",
        "model.fit(x, y, epochs=25, validation_data=(x_validation, y_validation), batch_size=5, callbacks=[tensorboard])\r\n",
        "model.save(\"ViolenceDetection.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}